{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRW+453LW9phxYrY9Nb97C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScreanReaderAI/ScreenReaderAIBackend/blob/main/JAWS_Command.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate huggingface_hub\n",
        "!huggingface-cli login\n",
        "\n",
        "\n",
        "from transformers import GPT2TokenizerFast, Trainer, TrainingArguments, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the DataFrame with your dataset\n",
        "data = {\n",
        "    \"Category\": [\"Web Browsing\", \"Document Editing\", \"File Management\"],\n",
        "    \"Software\": [\"Chrome\", \"Word\", \"Windows Explorer\"],\n",
        "    \"Question\": [\n",
        "        \"How do I open a new tab in Chrome?\",\n",
        "        \"How do I format text in Word?\",\n",
        "        \"How do I create a new folder?\"\n",
        "    ],\n",
        "    \"Step-by-Step Instructions\": [\n",
        "        \"Press CTRL+T to open a new tab.\",\n",
        "        \"Press ALT+H to access the Home ribbon, then F for font options.\",\n",
        "        \"Press CTRL+SHIFT+N to create a new folder.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Prepare the Data\n",
        "class JAWSDataset(Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def prepare_data(df):\n",
        "    # Load GPT-2 tokenizer\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "\n",
        "    # Ensure the pad token is properly set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    input_ids_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Tokenize the data\n",
        "    for _, row in df.iterrows():\n",
        "        input_text = row['Question']\n",
        "        output_text = row['Step-by-Step Instructions']\n",
        "\n",
        "        input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "        output_ids = tokenizer.encode(output_text, add_special_tokens=True)\n",
        "\n",
        "        input_ids_list.append(torch.tensor(input_ids, dtype=torch.long))\n",
        "        labels_list.append(torch.tensor(output_ids, dtype=torch.long))\n",
        "\n",
        "    # Pad the sequences so they all have the same length\n",
        "    input_ids_padded = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=-100)  # Padding label with -100 for loss calculation\n",
        "\n",
        "    return input_ids_padded, labels_padded, tokenizer\n",
        "\n",
        "# Step 3: Prepare Dataset and Trainer\n",
        "if __name__ == \"__main__\":\n",
        "    input_ids_padded, labels_padded, tokenizer = prepare_data(df)\n",
        "    dataset = JAWSDataset(input_ids_padded, labels_padded)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,  # Log every 10 steps\n",
        "        save_steps=50      # Save checkpoint every 50 steps\n",
        "    )\n",
        "\n",
        "    # Load pre-trained GPT-2 model\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Define a data collator that will dynamically pad the inputs and labels for each batch\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We are not using masked language modeling, as we are training GPT-2\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51dreSf5mUZ7",
        "outputId": "c5c849d9-a3e3-4cf3-e230-db6088f2e1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "        !pip install transformers datasets accelerate huggingface_hub\n",
        "!huggingface-cli login\n",
        "\n",
        "\n",
        "from transformers import GPT2TokenizerFast, Trainer, TrainingArguments, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the DataFrame with your dataset\n",
        "data = {\n",
        "    \"Category\": [\"Web Browsing\", \"Document Editing\", \"File Management\"],\n",
        "    \"Software\": [\"Chrome\", \"Word\", \"Windows Explorer\"],\n",
        "    \"Question\": [\n",
        "        \"How do I open a new tab in Chrome?\",\n",
        "        \"How do I format text in Word?\",\n",
        "        \"How do I create a new folder?\"\n",
        "    ],\n",
        "    \"Step-by-Step Instructions\": [\n",
        "        \"Press CTRL+T to open a new tab.\",\n",
        "        \"Press ALT+H to access the Home ribbon, then F for font options.\",\n",
        "        \"Press CTRL+SHIFT+N to create a new folder.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Prepare the Data\n",
        "class JAWSDataset(Dataset):\n",
        "    def __init__(self, input_ids, labels):\n",
        "        self.input_ids = input_ids\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "def prepare_data(df):\n",
        "    # Load GPT-2 tokenizer\n",
        "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "\n",
        "    # Ensure the pad token is properly set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    input_ids_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Tokenize the data\n",
        "    for _, row in df.iterrows():\n",
        "        input_text = row['Question']\n",
        "        output_text = row['Step-by-Step Instructions']\n",
        "\n",
        "        input_ids = tokenizer.encode(input_text, add_special_tokens=True)\n",
        "        output_ids = tokenizer.encode(output_text, add_special_tokens=True)\n",
        "\n",
        "        input_ids_list.append(torch.tensor(input_ids, dtype=torch.long))\n",
        "        labels_list.append(torch.tensor(output_ids, dtype=torch.long))\n",
        "\n",
        "    # Pad the sequences so they all have the same length\n",
        "    input_ids_padded = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=-100)  # Padding label with -100 for loss calculation\n",
        "\n",
        "    return input_ids_padded, labels_padded, tokenizer\n",
        "\n",
        "# Step 3: Prepare Dataset and Trainer\n",
        "if __name__ == \"__main__\":\n",
        "    input_ids_padded, labels_padded, tokenizer = prepare_data(df)\n",
        "    dataset = JAWSDataset(input_ids_padded, labels_padded)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,  # Log every 10 steps\n",
        "        save_steps=50      # Save checkpoint every 50 steps\n",
        "    )\n",
        "\n",
        "    # Load pre-trained GPT-2 model\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Define a data collator that will dynamically pad the inputs and labels for each batch\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We are not using masked language modeling, as we are training GPT-2\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    model.save_pretrained(\"./trained_jaws_model\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g-7ihjx3mCML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(\"/content/finai_model\")\n",
        "tokenizer.save_pretrained(\"/content/finai_model\")\n",
        "\n",
        "# Compress the saved model files into a .zip\n",
        "!zip -r finai_model.zip /content/finai_model\n",
        "\n",
        "# Download the zipped model file to your local computer\n",
        "from google.colab import files\n",
        "files.download(\"finai_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "mu4nP5mI4PND",
        "outputId": "3dbdd4c0-28ad-4f0c-89ff-3e1110cf9d04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/finai_model/ (stored 0%)\n",
            "  adding: content/finai_model/merges.txt (deflated 53%)\n",
            "  adding: content/finai_model/config.json (deflated 52%)\n",
            "  adding: content/finai_model/model.safetensors\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: finai_model.zip",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b400ebd4189c>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Download the zipped model file to your local computer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finai_model.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: finai_model.zip"
          ]
        }
      ]
    }
  ]
}