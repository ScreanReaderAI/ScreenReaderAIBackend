#!/usr/bin/env python3
import os
from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments
from datasets import load_dataset
from huggingface_hub import login
import torch
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence

# Set Hugging Face token from environment variables
huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
login(token=huggingface_token)

# Load dataset
dataset = load_dataset("path_to_your_dataset")  # Replace with actual dataset path

# Tokenize the dataset
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

input_ids_list = []
labels_list = []
for row in dataset["train"]:
    input_text = row["Question"]
    output_text = row["Step-by-Step Instructions"]
    input_ids = tokenizer.encode(input_text, add_special_tokens=True)
    output_ids = tokenizer.encode(output_text, add_special_tokens=True)
    input_ids_list.append(torch.tensor(input_ids, dtype=torch.long))
    labels_list.append(torch.tensor(output_ids, dtype=torch.long))

input_ids_padded = pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)
labels_padded = pad_sequence(labels_list, batch_first=True, padding_value=-100)

# Create custom dataset class
class JAWSDataset(Dataset):
    def __init__(self, input_ids, labels):
        self.input_ids = input_ids
        self.labels = labels

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'labels': self.labels[idx]
        }

# Initialize dataset
dataset = JAWSDataset(input_ids_padded, labels_padded)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=50
)

# Load pre-trained model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

# Train model
trainer.train()

# Save the trained model
model.save_pretrained("./trained_jaws_model")
tokenizer.save_pretrained("./trained_jaws_model")
